{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMfH9RlySDEi",
        "outputId": "7dcb8ff1-315f-46f8-c99b-c6e7ae7b7c0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "layer = nn.Linear(40, 10)\n",
        "\n",
        "layer.weight.data *= 6 ** 0.5\n",
        "torch.zero_(layer.bias.data)\n",
        "\n",
        "# Initializing random weights through kaiming he transformation by input features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.init.kaiming_uniform_(layer.weight)\n",
        "nn.init.zeros_(layer.bias)\n",
        "\n",
        "# same as we did before to change to kaiming he for relu activations only as it cancel out 50% of neurons if it is less than or equal to 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri0tBNvGY8X3",
        "outputId": "b6f8105c-d607-4392-c823-b491ae0b24d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def use_he_init(module):\n",
        "  if isinstance(module, nn.Linear):\n",
        "    nn.init.kaiming_uniform_(module.weight)\n",
        "    nn.init.zeros_(module.bias)\n",
        "\n",
        "model = nn.Sequential(nn.Linear(50, 40), nn.ReLU(), nn.Linear(40,1), nn.ReLU())\n",
        "model.apply(use_he_init)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaWfGqZCcqyJ",
        "outputId": "4b475a61-f342-4edb-fc60-15c001c67e7f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=50, out_features=40, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=40, out_features=1, bias=True)\n",
              "  (3): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.2\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(50, 40),\n",
        "    nn.LeakyReLU(negative_slope = alpha)\n",
        ")\n",
        "\n",
        "nn.init.kaiming_uniform_(model[0].weight, alpha, nonlinearity = 'leaky_relu')\n",
        "\n",
        "# Leaky ReLU used to reduce vanishing gradient if the linear output gives too much negative outputs by setting it up using alpha parameter to get slight slope over 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfsoDK0XeKjY",
        "outputId": "7e9485a2-7fd0-4534-dc49-5547f1521974"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0385,  0.2261,  0.0679,  ...,  0.1450,  0.2352,  0.2870],\n",
              "        [-0.0147, -0.2404, -0.3003,  ...,  0.0973, -0.1113, -0.2559],\n",
              "        [ 0.3144,  0.2840,  0.1420,  ...,  0.2951,  0.1464,  0.0474],\n",
              "        ...,\n",
              "        [-0.1000,  0.1273, -0.0369,  ...,  0.1679, -0.1796,  0.1265],\n",
              "        [ 0.0923,  0.2405,  0.0134,  ..., -0.0158, -0.1935,  0.0381],\n",
              "        [-0.1459,  0.3137,  0.3157,  ...,  0.1535,  0.0735,  0.3239]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.BatchNorm1d(1 * 28 * 28),\n",
        "    nn.Linear(1 * 28 * 28, 300),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(300),\n",
        "    nn.Linear(300, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(100),\n",
        "    nn.Linear(100, 10)\n",
        ")\n",
        "\n",
        "# we used Batchnorm to normalization of values and dont needed external normalization"
      ],
      "metadata": {
        "id": "Ag_sXCo3m_mk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict(model[1].named_parameters()).keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaSXMrJyQfNv",
        "outputId": "4d749a60-63e6-4361-a943-440f230286d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['weight', 'bias'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict(model[1].named_buffers()).keys()\n",
        "\n",
        "# The named buffer parameter contains the currently trained mean, variance and batch count of the batchnorm training until now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MwmSJGTQqp5",
        "outputId": "fcd6d3e2-3da3-4db8-89c3-99adea99a6a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['running_mean', 'running_var', 'num_batches_tracked'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.BatchNorm1d(1 * 28 * 28),\n",
        "    nn.Linear(1 * 28 * 28, 300, bias = False),\n",
        "    nn.BatchNorm1d(300),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(300, 100, bias = False),\n",
        "    nn.BatchNorm1d(100),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(100, 10)\n",
        ")\n",
        "\n",
        "# Here we used batchnorm1d before ReLU activation as it normalize the layer values to center around mean 0 which gives uniformly distributed instances\n",
        "# We need to remove the bias during linear layer as it assigns bias term for each instance which also again given at batchnorm function so it can reduce computation again"
      ],
      "metadata": {
        "id": "0gDjaoN2SZTA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor([\n",
        "    [1.0, 2.0, 3.0],\n",
        "    [100.0, 200.0, 300.0]],\n",
        "    dtype = torch.float32)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPiVynRaToC6",
        "outputId": "299ec984-216f-4a68-a182-6cd628f49fdb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  1.,   2.,   3.],\n",
            "        [100., 200., 300.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn = nn.BatchNorm1d(num_features=3)\n",
        "bn.train()\n",
        "out_bn = bn(data)\n",
        "print(out_bn)\n",
        "\n",
        "# Batchnorm chooses the column wise minimum value if both are positive or negative"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBYliGIwahG1",
        "outputId": "fb702120-f52f-4a8a-dc3b-9b78a3cae93a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0000, -1.0000, -1.0000],\n",
            "        [ 1.0000,  1.0000,  1.0000]], grad_fn=<NativeBatchNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ln = nn.LayerNorm(normalized_shape = 3)\n",
        "out_ln = ln(data)\n",
        "print(out_ln)\n",
        "# Layernorm normalize the row instance by calculating againts the mean value from the vector so 2 is average then it becomes 0 as middle point\n",
        "# Calculates seperately for each instances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5Po3dHla24w",
        "outputId": "3b5c9a7f-1732-4947-d2e7-67f293ca4244"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.2247,  0.0000,  1.2247],\n",
            "        [-1.2247,  0.0000,  1.2247]], grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.randn(32, 3, 100, 200)\n",
        "layer_norm = nn.LayerNorm([100, 200])\n",
        "result = layer_norm(inputs)\n",
        "print(result.shape)\n",
        "\n",
        "# For a tensor image with batch 32 , channel 3 rgb and height and width of 100 X 200 we have to specify the layer_normalization dimension to normalize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ERdBYxTnbH3H",
        "outputId": "64afd05f-6263-4337-a245-d1de6c6a635f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 3, 100, 200])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_norm = nn.LayerNorm([3, 100, 200])\n",
        "result = layer_norm(inputs)\n",
        "\n",
        "# Now this will calculate the dimensions from channel like Red,gree and blue as a whole single image rather than seperate colors"
      ],
      "metadata": {
        "id": "TFIVo_u_cYyW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = nn.LSTM(input_size = 10, hidden_size= 20, num_layers = 2)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "input_seq = torch.randn(50, 32, 10)\n",
        "target = torch.randn(50, 32, 20)\n",
        "\n",
        "model.train()\n",
        "\n",
        "output, _ = model(input_seq)\n",
        "loss = criterion(output, target)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
        "print(f\"Total norm Before clipping:{grad_norm:.4f}\")\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "\n",
        "# Grad Clipping used to rescale the output grad of each layer without changing direction during backpropagation by assigning between fixed scale\n",
        "# This is particularly very usefull in LSTM and RNN as the neurons pass through each other seevral times which may cause exploading gradients\n"
      ],
      "metadata": {
        "id": "s423ydE0hopc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356c0ac8-28ce-4b8d-e0be-a405aa335d2e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total norm Before clipping:0.0313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nyTUdec3Xuk"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}